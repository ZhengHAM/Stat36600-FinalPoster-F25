---
title: Classification using Logistic Regression and Machine Learning Models
subtitle: |
  Project 3 Report
author: HOWARD MA
output:
  pdf_document:
    toc: false
  html_document:
    toc: false
    code-overflow: wrap
    code-fold: false
    theme: spacelab
urlcolor: blue
---

## 1. Introduction

This report will examine the wine quality dataset by Cortez et al. (2009). The purpose is to build predictive models that classify each wine as wither `GOOD` or `BAD` based on the attributes introduced in the dataset, such as acidity, sugar, and other chemicals and content.

We will begin with a brief overview of the dataset and EDA summaries, then split the data into training and test sets. We will use a logistic regression model as the baseline classifier, followed by ROC analysis. We will then evaluate classification performance through a confusion matrix and misclassification rate. Additional machine learning models will also be explored near the end of the report.

---

## 2. Data Overview and Exploratory Analysis

We begin with a basic inspection and overview of the dataset, mainly focusing on dimensions, structure, completeness, and some initial summary statistics. This section will set as a foundation for future analysis in the report.

```{r}
suppressMessages(library(tidyverse))

df <- read.csv("wineQuality.csv", stringsAsFactors = TRUE)

# Basic overview
dim(df)
colSums(is.na(df))
```

The dataset contains 6497 rows and 12 columns, providing a relatively large sized set that should be suitable for the analysis. In addition, there does not appear to be any missing values in any column. This shows that the dataset is complete and no variables need to be removed prior to analysis due to insufficiency.

```{r}
summary(df)
```
From the summary, we can make note of some general observations:

1. The dataset contains a mix of acidity measurements, sulfur dioxide levels, alcohol content, etc., with each spanning fairly wide numeric ranges, suggesting a diverse set of chemical content across the wines.

2. Several predictors like sugar and sd exhibit right skews:
a. Residual sugar shows substantial skew, as it ranges from 0.6 to 65.8, with a mean of 5.44 while the median is only 3. This suggesting that a small subset of unusually sweet wines.
b. Free and total sd both have very wide ranges, with their maximum values (289 and 440) far exceeding their medians (29 and 118) and mean (30.53 and 115.7), suggesting there may be some outliers.

3. Label shows an imbalance, with more `GOOD` than `BAD`, which is kept in mind for later analysis and classification verification.

Overall, the dataset we have seem to be clean, free of missing or invalid values, and aligns with expected chemical ranges, so no immediate removal of any predictor variables is necessary.

---

## 3. Dataset Splitting

Before fitting the model, we divided the dataset into training and test sets to allow for independent training and evaluation. For the split, we decided to use the common 70/30 split where 70% of the data is used for training and 30% is saved for testing later on. This helps ensure the evaluation can reflect generalized data rather than potentially overfitting.

```{r}
set.seed(100)
s <- sample(nrow(df), round(0.7 * nrow(df)))
df.train <- df[s, ]
df.test <- df[-s, ]
```

After splitting, we do a quick check for the classification distribution in each subset to verify that the original imbalance between `GOOD` and `BAD` is preserved.

```{r}
cat("Total number of rows of training set:", nrow(df.train))
table(df.train$label)
cat("Total number of rows of test set:", nrow(df.test))
table(df.test$label)
```

The proportions of `GOOD` and `BAD` in both subsets seem to remain similar to the overall dataset, which is important for maintaining consistent model behavior and ensuring that ROC analysis work as intended. This split will serve as the foundation for training the logistic regression model and evaluating its predictive performance on unseen data.

---

## 4. Logistic Regression Model

We begin by fitting a logistic regression model using the predictor variables. We will focus on estimated probabilities, ROC threshold selection, and classification performance on the test set. With prior observation of imbalance, the default probability cutoff of 0.5 is not be optimal, thus we rely on ROC analysis to evaluate classifier performance across all possible thresholds. Which combined with AUC summerizes how well the model sperates the two classes. We will then use Youden's J statistic to select the best threshold to convert into labels. In the end, displaying the confusion matrix and misclassification rate will provide a clear view of performance.


```{r}
suppressMessages(library(pROC))
glm.out <- glm(label ~ ., data = df.train, family = binomial)
summary(glm.out)
glm.prob <- predict(glm.out, newdata = df.test, type = "response")
glm.roc <- roc(df.test$label, glm.prob)
plot(glm.roc, col = "pink", xlim = c(1, 0), ylim = c(0, 1))
cat("AUC for logistic regression:", round(glm.roc$auc, 3))
```

From the ROC curve, we can see a clear separation between the two classes: the curve lies substantially above the baseline, indicating predictive performance is certainly better. The resulting AUC value is 0.778, suggesting that approximately 77.8% of the time, the model assigns a higher probability to a randomly chosen `GOOD` wine than to a randomly chosen `BAD` wine.

```{r}
glm.J.roc <- glm.roc$sensitivities + glm.roc$specificities - 1
glm.J.best.roc <- round(glm.roc$thresholds[which.max(glm.J.roc)], 3)
cat("Optimum threashold for glm:", glm.J.best.roc)
```

From the ROC analysis and using Youden's J statistic we are able to determine that the optimum classification threshold here is 0.639, which will be used for confusion matrix and MCR.

```{r}
glm.pred <- ifelse(glm.prob > glm.J.best.roc, "GOOD", "BAD")
glm.table <- table(glm.pred, df.test$label)
glm.table
glm.mcr <- mean(glm.pred != df.test$label)
cat("The MCR for glm is:", round(glm.mcr, 3))
```

Applying the threshold yielded the above results. From the table we can see that the model correctly classifies 874 `GOOD` wines and `506` BAD wines, while misclassifying 389 `GOOD` wines and 180 `BAD` wines. The overall misclassification rate is 0.292, meaning that the logistic regression model correctly predicts roughly 70.8% of all wines in the test set. This performance aligns well with the AUC value of 0.778, which already suggested reasonably strong separation between the two classes. The model is noticeably better at identifying `GOOD` wines, though at the cost of a higher number of false negatives for the `BAD` wines.

In summary, the logistic regression model provides solid baseline predictive performance, achieving an AUC of 0.778 and a test-set misclassification rate of about 29.2%. These results establish a useful benchmark for comparison when we explore additional machine learning models in the following section. To keep the analysis focused, though, we will not pursue best-subset optimizations, as the full model already seems to offer a clear and easily interpretable baseline.
---

## 5. Decision Tree Model

In this section, we will try to fit a decision tree classifier model using the predictor variables to evaluate whether this flexible model can improve on the baseline performance of the logistic regression model. The process flow is similar to before where we will focus on prediction probabilities, ROC analysis, and MCR.

```{r}
suppressMessages(library(rpart))
suppressMessages(library(rpart.plot))
rpart.out <- rpart(label ~ ., data = df.train)
rpart.prob <- predict(rpart.out, newdata = df.test, type = "prob")[, "GOOD"]
rpart.roc <- roc(df.test$label, rpart.prob)
plot(rpart.roc, col = "pink", xlim = c(1, 0), ylim = c(0, 1))
cat("AUC for decision tree model:", round(rpart.roc$auc, 3))
```

The ROC curve for the decision tree model shows moderate separation between the GOOD and BAD classes, with an AUC of 0.738. This performance is slightly weaker than the logistic regression model, suggesting that the single tree does not seem to capture as much structure in the data. Nevertheless, the ROC curve still rises above the diagonal baseline, indicating meaningful predictive ability and allowing us to determine an optimal threshold using Youden’s J statistic.

```{r}
rpart.J.roc <- rpart.roc$sensitivities + rpart.roc$specificities - 1
rpart.J.best.roc <- round(rpart.roc$thresholds[which.max(rpart.J.roc)], 3)
cat("Optimum threashold for decision tree:", rpart.J.best.roc)
rpart.pred <- ifelse(rpart.prob > rpart.J.best.roc, "GOOD", "BAD")
table.rpart <- table(rpart.pred, df.test$label)
table.rpart
mcr.rpart <- mean(rpart.pred != df.test$label)
cat("The MCR for decision tree is:", round(mcr.rpart, 3))
```

Using the threshold of 0.489, the decision tree correctly classifies 999 `GOOD` wines and 424 `BAD` wines, resulting in a misclassification rate of 27%. Compared to the logistic regression before, decision tree shows slightly lower AUC but a modest improvement in MCR, indicating that its flexibility helps capture some nonlinear structure in the data. However, the tree also produces more false positives and false negatives overall, reflecting instability and limited depth of this model.

---

## 6. Random Forest Model

In this section, we will fit a random forest classifier to see whether a more robust method can improve upon the earlier models. As before, we will evaluate performance using predicted probabilities, ROC analysis, and misclassification rate.

```{r}
suppressMessages(library(randomForest))
set.seed(100)
rf.out <- randomForest(label ~ ., data = df.train, importance = TRUE)
rf.prob <- predict(rf.out, newdata = df.test, type = "prob")[, "GOOD"]
rf.roc <- roc(df.test$label, rf.prob)
plot(rf.roc, col = "pink", xlim = c(1, 0), ylim = c(0, 1))
cat("AUC for random forest Model:", round(rf.roc$auc, 3))
```

The ROC curve this time for the random forest model shows a clear improvement over both logistic regression and the decision tree. The curve rises steeply toward the top left, and the AUC of 0.881 indicates strong class separation and substantially better predictive accuracy. This suggests that the random forest approach is capturing nonlinear patterns and interactions better than the earlier models.

```{r}
rf.J.roc <- rf.roc$sensitivities + rf.roc$specificities - 1
rf.J.best.roc <- round(rf.roc$thresholds[which.max(rf.J.roc)], 3)
cat("Optimum threashold for random forest:", rf.J.best.roc)
rf.pred <- ifelse(rf.prob > rf.J.best.roc, "GOOD", "BAD")
table.rf <- table(rf.pred, df.test$label)
table.rf
mcr.rf <- mean(rf.pred != df.test$label)
cat("The MCR for this model is:", round(mcr.rf, 3))
```

Using the optimal threshold of 0.603, the random forest model correctly classifies 1016 `GOOD` wines and 552 `BAD` wines, giving a misclassification rate of around 19.5%, the lowest among all models tested. This substantial improvement reflects the strength of the random forest approach, which aggregates many decision trees and captures nonlinear relationships that simpler models miss.

---

## 7. SVM Model

In this section, we will use an SVM model.

```{r}
suppressMessages(library(e1071))
svm.out <- svm(label ~ ., data = df.train, kernel = "linear", probability = TRUE)
svm.prob <- attr(predict(svm.out, newdata = df.test, probability = TRUE), 
  "probabilities")[, "GOOD"]
svm.roc <- roc(df.test$label, svm.prob)
plot(svm.roc, col = "pink", xlim = c(1, 0), ylim = c(0, 1))
cat("AUC for SVM model:", round(svm.roc$auc, 3))
```

Using similar methods, we are able to obtain the AUC value for the ROC analysis of the SVM model, which comes down to 0.7768, indicating moderate class separation. The curve rises steadily above the diagonal baseline, suggesting that the linear SVM captures some useful structure in the data but does not improve meaningfully over the baseline GLM.

```{r}
svm.J.roc <- svm.roc$sensitivities + svm.roc$specificities - 1
svm.J.best.roc <- round(svm.roc$thresholds[which.max(svm.J.roc)], 3)
cat("Optimum threashold for SVM:", svm.J.best.roc)
svm.pred <- ifelse(svm.prob > svm.J.best.roc, "GOOD", "BAD")
table.svm <- table(svm.pred, df.test$label)
table.svm
mcr.svm <- mean(svm.pred != df.test$label)
cat("The MCR for SVM model is:", round(mcr.svm, 3))
```

Using the optimal threshold of 0.648, the SVM model correctly identifies 878 `GOOD` wines and 502 `BAD` wines, resulting in a misclassification rate of 29.2%. This accuracy is nearly identical to logistic regression and weaker than both the decision tree and random forest. Overall, SVM provides a solid but mid-range model, performing better than simple linear methods in some cases but not matching the flexibility or accuracy of random forest approaches.

---

## 8. Model Comparison and Conclusion

We will now compile the AUC values and MCR for all the models together in a data frame, and all the ROC curves in one graph. This allows for a direct comparison of performance across different models.

```{r}
model.names <- c("Logistic Regression", "Decision Tree", "Random Forest", 
  "SVM Model")
model.AUC <- c(round(glm.roc$auc, 3),
               round(rpart.roc$auc, 3),
               round(rf.roc$auc, 3),
               round(svm.roc$auc, 3))

model.MCR <- c(round(glm.mcr, 3),
               round(mcr.rpart, 3),
               round(mcr.rf, 3),
               round(mcr.svm, 3))

results.df <- data.frame(Model = model.names,
                         AUC = model.AUC,
                         MisclassificationRate = model.MCR)
results.df
```

```{r}
plot(glm.roc, col = "blue", main = "ROC Curves -- All Models",
     xlim = c(1,0), ylim = c(0,1))
lines(rpart.roc, col = "green")
lines(rf.roc, col = "pink")
lines(svm.roc, col = "red")
```

Across the models ran, random forest clearly provides the strongest predictive performance. Logistic regression establishes a reasonable baseline with an AUC of 0.778 and a misclassification rate of 29.2%, while the decision tree shows a slight improvement in accuracy but lower AUC than baseline, reflecting its limited modeling flexibility. In contrast, random forest achieves both the highest AUC and the lowest misclassification rate, indicating substantially better class separation and overall predictive accuracy. These improvements are also visible in the overlaid ROC curves, where the random forest curve consistently dominates the others.

Overall, this report shows results that indicate random forest method is much more effective for this dataset. Random forest’s ability to aggregate many trees allows it to capture nonlinear relationships and variable interactions better than simpler models, leading to the best classification results. Based on the combined evidence from AUC, threshold-based accuracy, and ROC behavior, random forest is the preferred model for predicting wine quality in this analysis.